BLEU Result for lv_en test split: {'score': 22.11897087809183, 'counts': [7281, 4115, 2436, 1377], 'totals': [14183, 12554, 10926, 9308], 'precisions': [51.33610660650074, 32.77839732356221, 22.29544206479956, 14.79372582724538], 'bp': 0.8103802396533598, 'sys_len': 14183, 'ref_len': 17165}
BLEU Result for mn_en test split: {'score': 9.691976013850546, 'counts': [9742, 3436, 1489, 675], 'totals': [24813, 23054, 21295, 19536], 'precisions': [39.26167734655221, 14.90413811052312, 6.99225170227753, 3.455159705159705], 'bp': 0.8888369629210789, 'sys_len': 24813, 'ref_len': 27737}
BLEU Result for ta_en test split: {'score': 4.799525204317135, 'counts': [1452, 447, 175, 70], 'totals': [5980, 5194, 4413, 3640], 'precisions': [24.2809364548495, 8.606083943011166, 3.965556310899615, 1.9230769230769231], 'bp': 0.7596356352549682, 'sys_len': 5980, 'ref_len': 7624}
